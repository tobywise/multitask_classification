{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEG data loading function examples\n",
    "\n",
    "This notebook demonstrates the use of the data loading function for MEG data (`load_MEG_dataset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 15:45:16.062305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-17 15:45:16.062495: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code')\n",
    "from load_data import load_MEG_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add code directory to path\n",
    "\n",
    "This is necessary because the code for the data loading function is not in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the function\n",
    "\n",
    "The function `load_MEG_dataset` downloads, loads, and preprocesses the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_MEG_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mload_MEG_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubject_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'individual'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'numpy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrial_data_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2D'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_location\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcenter_timepoint\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwindow_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpca_n_components\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Downloads and loads MEG data from the specified subject ids. Data is returned as a generator of batches, if batch is True, or\n",
      "a generator with a single batch if batch is False.\n",
      "\n",
      "Data is returned as an X array representing the MEG data and a y array representing the labels.\n",
      "\n",
      "Args:\n",
      "    subject_ids (list): List of subject ids to load as strings, e.g. (['001', '002']).\n",
      "    mode (str, optional): How to organise the data, can be one of 'individual', 'concatenate', or 'stack'.\n",
      "    If 'individual', a list of datasets is returned, each representing a single subject. If 'concatenate', all datasets are concatenated\n",
      "    together and returned as a single dataset. If 'stack', all datasets are stacked together and returned as a single dataset.\n",
      "    Defaults to 'individual'.\n",
      "    output_format (str, optional): Format of the data, can be one of 'numpy' or 'tf'. If 'numpy', the data is returned as a numpy array. If\n",
      "    'tf', data is returned as a tensorflow dataset. Defaults to 'numpy'.\n",
      "    trial_data_format (str, optional): Format of the trial data, can be one of '2D' or '1D'. If '2D', the data for each trial is\n",
      "    returned as a 2D array, where the first dimension is the channel number and the second dimension is the time index. If '1D', the data\n",
      "    for each trial is returned as a 1D array, with channels and time points flattened. Defaults to '2D'.\n",
      "    data_location (str, optional): Location to store the data. Defaults to \"./data/\".\n",
      "    shuffle (bool, optional): If True, shuffles the data across trials. Defaults to False.\n",
      "    pca_n_components (int, optional): Number of components to use when running PCA. If None, PCA is not performed. Defaults to None.\n",
      "    training (bool, optional): If True, returns training data. Defaults to True.\n",
      "    train_test_split (float, optional): Percentage of data to use for training. Defaults to 0.75.\n",
      "    center_timepoint (int, optional): Index of the center time point for the classifier, post stimulus\n",
      "    onset. Defaults to 20.\n",
      "    window_width (List[int], optional): Window of data to use for the classifier. Defaults to [-5, 6].\n",
      "    batch_size (int, optional): Batch size, to be used if using TF format. Defaults to 32.\n",
      "    seed (int, optional): Random seed. Defaults to 0.\n",
      "\n",
      "Returns:\n",
      "    Union[Tuple[List[np.ndarray], List[np.ndarray]], Tuple[np.ndarray, np.ndarray], Dataset]: X and y data, or a generator of batches.\n",
      "\u001b[0;31mFile:\u001b[0m      /mnt/c/Users/tobyw/OneDrive - King's College London/multitask_classification/code/load_data.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "?load_MEG_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic usage\n",
    "\n",
    "The function can load in data for any number of subjects, which can be specified in the `subject_ids` argument using a list of subject IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Single subject\n",
    "X, y = load_MEG_dataset(['001'])\n",
    "\n",
    "# Two subjects\n",
    "X, y = load_MEG_dataset(['001', '002'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format\n",
    "\n",
    "The function can return the data as either a Numpy array (more useful for e.g. scikit-learn) or a batched Tensorflow dataset (more useful for neural networks).\n",
    "\n",
    "The data format is specified using the `output_format` parameter.\n",
    "\n",
    "If `output_format` is `'numpy'`, the function returns a tuple of two Numpy arrays: `(data, labels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "(1350, 272, 11)\n",
      "(1350,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy')\n",
    "print(X.shape)  # (1350, 272, 11)\n",
    "print(y.shape)  # (1350, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output format is `'tf'`, the function returns a batched Tensorflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 15:49:06.922913: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-17 15:49:06.923250: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-17 15:49:06.923468: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-KC0FGA6): /proc/driver/nvidia/version does not exist\n",
      "2022-05-17 15:49:06.925669: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec={'image': TensorSpec(shape=(None, 272, 11), dtype=tf.float64, name=None), 'label': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='tf')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with multiple subjects\n",
    "\n",
    "Data for multiple subjects can be returned in three different ways, depending on the `mode` parameter.\n",
    "\n",
    "1.  `mode='individual'`: The function returns a list of datasets, one for each subject.\n",
    "2.  `mode='concatenate'`: The function returns a single dataset, where the data and labels are concatenated across subjects.\n",
    "3.  `mode='stack'`: The function returns a single dataset, where the data and labels are stacked across subjects in an additional dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "2 2\n",
      "(675, 272, 11) (675,)\n",
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "(1350, 272, 11) (1350,)\n",
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "(2, 675, 272, 11) (2, 675)\n"
     ]
    }
   ],
   "source": [
    "# Individual - a list of numpy arrays\n",
    "X, y = load_MEG_dataset(['001', '002'], mode='individual', output_format='numpy')\n",
    "print(len(X), len(y))  # Lists with two elements\n",
    "print(X[0].shape, y[0].shape)  # (675, 272, 11) (675,)\n",
    "\n",
    "# Concatenate - a single numpy array\n",
    "X, y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy')\n",
    "print(X.shape, y.shape)  # (1350, 272, 11) (1350, 2)\n",
    "\n",
    "# Stack - a single numpy array, with subjects represented in the first dimension\n",
    "X, y = load_MEG_dataset(['001', '002'], mode='stack', output_format='numpy')\n",
    "print(X.shape, y.shape)  # (2, 675, 272, 11) (2, 675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial data format\n",
    "\n",
    "The data for each trial can be returned as either a flattened 1D array or a 2D array with shape `(n_channels, n_samples)`. This can be set using the `trial_format` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "(1350, 2992) (1350,)\n",
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "(1350, 272, 11) (1350,)\n"
     ]
    }
   ],
   "source": [
    "# 1D format\n",
    "X, y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy', trial_data_format='1D')\n",
    "print(X.shape, y.shape)  # (1350, 2992) (1350, 2)\n",
    "\n",
    "# 2D format\n",
    "X, y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy', trial_data_format='2D')\n",
    "print(X.shape, y.shape)  # (1350, 272, 11) (1350, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)\n",
    "\n",
    "The function can also run PCA on the data. This can be done using the `pca_n_components` parameter - if this is set to `None` (the default), no PCA is performed. Otherwise, PCA is performed with the number of components specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Running PCA\n",
      "PCA complete\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Running PCA\n",
      "PCA complete\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "(1350, 30, 11)\n"
     ]
    }
   ],
   "source": [
    "# PCA with 30 components\n",
    "X, y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy', trial_data_format='2D', pca_n_components=30)\n",
    "print(X.shape)  # (1350, 30, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the timepoints to use\n",
    "\n",
    "The center timepoint used for the data window can be set using the `center_timepoint` parameter. This is the sample index **after stimulus onset** that will be center of the data window. The default is 20 (i.e., 200ms after stimulus onset).\n",
    "\n",
    "The width of the window used for data extraction is specified using the `window_width` parameter. This is the number of samples before and after the center timepoint that will be included in the data window. This is specified as a tuple of the form (`samples before`, `samples after`). Note that `samples_after` must be specified as 1 greater than the actual number of samples as this includes the center of the window. The default is (-5, 6), i.e. 5 samples before and 5 samples after the center timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Running PCA\n",
      "PCA complete\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Running PCA\n",
      "PCA complete\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "(1350, 30, 21)\n"
     ]
    }
   ],
   "source": [
    "# With a window of 10 samples before and after\n",
    "X, y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy', trial_data_format='2D', pca_n_components=30, window_width=(-10, 11))\n",
    "print(X.shape)  # (1350, 30, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the data\n",
    "\n",
    "The function can also shuffle the trials. This can be done using the `shuffle` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7266/1492422129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_MEG_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'001'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'002'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concatenate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/tobyw/OneDrive - King's College London/multitask_classification/code/load_data.py\u001b[0m in \u001b[0;36mload_MEG_dataset\u001b[0;34m(subject_ids, mode, output_format, trial_data_format, data_location, center_timepoint, window_width, shuffle, pca_n_components, training, train_test_split, batch_size, seed)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"individual\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/tobyw/OneDrive - King's College London/multitask_classification/code/load_data.py\u001b[0m in \u001b[0;36mshuffle_data\u001b[0;34m(X, y, seed)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "X, y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy', trial_data_format='2D', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test sets\n",
    "\n",
    "The function can provide training and test data sets. The relative size of these datasets can be specified using the `train_test_split` parameter, which specifies the proportion of data to be used for training (by default, this is set to 0.75).\n",
    "\n",
    "The function can return either the training or test data set. This can be specified using the `training` parameter. If `training` is `True`, the function returns the training data set. If `training` is `False`, the function returns the test data set. Note that because the randomness is consistent, based on the `seed` parameter, the same training and test data set will be returned for each call to the function unless the `seed` parameter is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy')\n",
    "test_X, test_y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy')\n",
    "\n",
    "print(train_X.shape, train_y.shape)  # (1350, 272, 11) (1350, 2)\n",
    "print(test_X.shape, test_y.shape)  # (450, 272, 11) (450, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "If the data is returned as a batched Tensorflow dataset, the data can be batched using the `batch_size` parameter to determine the size of each batch. This batches across trials, so each batch contains a subset of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n",
      "Loading subject 001\n",
      "Data loaded\n",
      "Subject 001 complete\n",
      "--------------------------------------\n",
      "Loading subject 002\n",
      "Data loaded\n",
      "Subject 002 complete\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ds = test_X, test_y = load_MEG_dataset(['001', '002'], mode='concatenate', output_format='numpy', batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbaac61c67fc802a76c1c1a871fcbca294a8a22a8af54456f95cc9d53d810f07"
  },
  "kernelspec": {
   "display_name": "gps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
